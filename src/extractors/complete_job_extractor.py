"""
å…¨æ±‚äººå–å¾—å™¨ - ä¸‰è±é›»æ©ŸJPOSTINGç”¨
377ä»¶ã™ã¹ã¦ã®æ±‚äººã‚’å–å¾—ãƒ»è§£æã™ã‚‹
"""

import requests
from bs4 import BeautifulSoup
import re
from typing import List, Dict, Set
import time
from urllib.parse import urljoin, urlparse, parse_qs
import logging

class CompleteJobExtractor:
    def __init__(self):
        self.base_url = "https://progres02.jposting.net"
        self.session = requests.Session()
        self.session.headers.update({
            'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
        })
        self.all_jobs: List[Dict[str, str]] = []
        self.job_codes: Set[str] = set()
        
    def extract_all_jobs(self, start_url: str) -> List[Dict[str, str]]:
        """
        377ä»¶ã™ã¹ã¦ã®æ±‚äººã‚’å–å¾—
        """
        print("ğŸš€ å…¨æ±‚äººå–å¾—ã‚’é–‹å§‹...")
        
        # Step 1: ãƒ¡ã‚¤ãƒ³ãƒšãƒ¼ã‚¸ã‹ã‚‰ã™ã¹ã¦ã®æ±‚äººã‚³ãƒ¼ãƒ‰ã‚’åé›†
        self._collect_all_job_codes(start_url)
        
        print(f"ğŸ“Š ç™ºè¦‹ã—ãŸæ±‚äººã‚³ãƒ¼ãƒ‰æ•°: {len(self.job_codes)}ä»¶")
        
        # Step 2: å„æ±‚äººã®è©³ç´°æƒ…å ±ã‚’å–å¾—
        self._fetch_job_details()
        
        print(f"âœ… å–å¾—å®Œäº†: {len(self.all_jobs)}ä»¶ã®æ±‚äººãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—")
        return self.all_jobs
    
    def _collect_all_job_codes(self, url: str):
        """
        ã™ã¹ã¦ã®æ±‚äººã‚³ãƒ¼ãƒ‰ã‚’åé›†
        """
        try:
            print(f"ğŸ” æ±‚äººã‚³ãƒ¼ãƒ‰åé›†ä¸­: {url}")
            response = self.session.get(url, timeout=15)
            response.encoding = 'utf-8'
            
            soup = BeautifulSoup(response.text, 'html.parser')
            
            # æ±‚äººã¸ã®ãƒªãƒ³ã‚¯ã‚’å…¨ã¦å–å¾—
            for link in soup.find_all('a', href=True):
                href = link['href']
                
                # job_codeãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ãŒã‚ã‚‹ãƒªãƒ³ã‚¯ã‚’æ¢ã™
                if 'job_code=' in href:
                    # job_codeã‚’æŠ½å‡º
                    match = re.search(r'job_code=(\d+)', href)
                    if match:
                        job_code = match.group(1)
                        self.job_codes.add(job_code)
                        
                        # ãƒ‡ãƒãƒƒã‚°: æœ€åˆã®10ä»¶ã®ãƒªãƒ³ã‚¯ãƒ†ã‚­ã‚¹ãƒˆã‚’è¡¨ç¤º
                        if len(self.job_codes) <= 10:
                            link_text = link.get_text().strip()[:50]
                            print(f"  æ±‚äººã‚³ãƒ¼ãƒ‰{job_code}: {link_text}...")
            
            print(f"ğŸ“ ãƒ¡ã‚¤ãƒ³ãƒšãƒ¼ã‚¸ã‹ã‚‰ {len(self.job_codes)}ä»¶ã®æ±‚äººã‚³ãƒ¼ãƒ‰ã‚’ç™ºè¦‹")
            
            # ã‚‚ã—377ä»¶ã«æº€ãŸãªã„å ´åˆã€æ¤œç´¢ã‚„ãƒ•ã‚£ãƒ«ã‚¿ã‚’è©¦ã™
            if len(self.job_codes) < 300:
                print("âš ï¸ æ±‚äººæ•°ãŒå°‘ãªã„ãŸã‚ã€è¿½åŠ ã®æ¤œç´¢ã‚’å®Ÿè¡Œ...")
                self._try_additional_searches(url)
                
        except Exception as e:
            print(f"âŒ æ±‚äººã‚³ãƒ¼ãƒ‰åé›†ã‚¨ãƒ©ãƒ¼: {e}")
    
    def _try_additional_searches(self, base_url: str):
        """
        è¿½åŠ ã®æ¤œç´¢ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’è©¦è¡Œ
        """
        # ç•°ãªã‚‹æ¤œç´¢æ¡ä»¶ã‚„ã‚½ãƒ¼ãƒˆé †ã§è©¦è¡Œ
        search_params = [
            '',  # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ
            '?search=1',  # æ¤œç´¢å®Ÿè¡Œ
            '?sort=new',  # æ–°ç€é †
            '?sort=code', # ã‚³ãƒ¼ãƒ‰é †
        ]
        
        for param in search_params:
            try:
                test_url = base_url + param
                print(f"  ğŸ” è¿½åŠ æ¤œç´¢: {test_url}")
                
                response = self.session.get(test_url, timeout=10)
                response.encoding = 'utf-8'
                soup = BeautifulSoup(response.text, 'html.parser')
                
                original_count = len(self.job_codes)
                
                # æ–°ã—ã„æ±‚äººã‚³ãƒ¼ãƒ‰ã‚’æ¢ã™
                for link in soup.find_all('a', href=True):
                    href = link['href']
                    if 'job_code=' in href:
                        match = re.search(r'job_code=(\d+)', href)
                        if match:
                            self.job_codes.add(match.group(1))
                
                new_count = len(self.job_codes)
                if new_count > original_count:
                    print(f"    âœ… {new_count - original_count}ä»¶ã®è¿½åŠ æ±‚äººã‚’ç™ºè¦‹")
                
                time.sleep(0.5)  # ãƒ¬ãƒ¼ãƒˆåˆ¶é™
                
            except Exception as e:
                print(f"    âŒ è¿½åŠ æ¤œç´¢ã‚¨ãƒ©ãƒ¼: {e}")
    
    def _fetch_job_details(self):
        """
        å„æ±‚äººã®è©³ç´°æƒ…å ±ã‚’å–å¾—
        """
        print("ğŸ”„ æ±‚äººè©³ç´°æƒ…å ±ã‚’å–å¾—ä¸­...")
        
        for i, job_code in enumerate(sorted(self.job_codes), 1):
            try:
                # ãƒ—ãƒ­ã‚°ãƒ¬ã‚¹è¡¨ç¤º
                if i % 50 == 0 or i <= 10:
                    print(f"  ğŸ“‹ é€²æ—: {i}/{len(self.job_codes)} ({i/len(self.job_codes)*100:.1f}%)")
                
                job_url = f"{self.base_url}/pgmitsubishielectric/u/job.phtml?job_code={job_code}"
                
                response = self.session.get(job_url, timeout=10)
                
                # ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°ã‚’è¤‡æ•°è©¦è¡Œ
                encodings = ['utf-8', 'shift_jis', 'euc-jp', 'iso-2022-jp']
                soup = None
                
                for encoding in encodings:
                    try:
                        response.encoding = encoding
                        soup = BeautifulSoup(response.text, 'html.parser')
                        # æ–‡å­—åŒ–ã‘ãƒã‚§ãƒƒã‚¯: æ—¥æœ¬èªãŒæ­£ã—ãè¡¨ç¤ºã•ã‚Œã¦ã„ã‚‹ã‹
                        test_text = soup.get_text()[:500]
                        if 'ï¿½' not in test_text and any(char in test_text for char in 'ã‚ã„ã†ãˆãŠ'):
                            break
                    except:
                        continue
                
                if not soup:
                    # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯: ç”Ÿã®HTMLã‹ã‚‰æŠ½å‡ºã‚’è©¦è¡Œ
                    soup = BeautifulSoup(response.content, 'html.parser', from_encoding='shift_jis')
                
                # æ±‚äººæƒ…å ±ã‚’æŠ½å‡º
                job_info = self._parse_job_page(soup, job_url, job_code)
                
                if job_info:
                    self.all_jobs.append(job_info)
                    
                    # æœ€åˆã®æ•°ä»¶ã®è©³ç´°ã‚’ãƒ­ã‚°å‡ºåŠ›
                    if len(self.all_jobs) <= 5:
                        print(f"    âœ… {job_info['title'][:60]}...")
                
                # ãƒ¬ãƒ¼ãƒˆåˆ¶é™ï¼ˆ1ç§’é–“ã«2ãƒªã‚¯ã‚¨ã‚¹ãƒˆç¨‹åº¦ï¼‰
                time.sleep(0.5)
                
            except Exception as e:
                print(f"  âŒ æ±‚äººã‚³ãƒ¼ãƒ‰{job_code}ã®å–å¾—ã‚¨ãƒ©ãƒ¼: {e}")
                continue
    
    def _parse_job_page(self, soup: BeautifulSoup, url: str, job_code: str) -> Dict[str, str]:
        """
        å€‹åˆ¥æ±‚äººãƒšãƒ¼ã‚¸ã‹ã‚‰æƒ…å ±ã‚’æŠ½å‡º
        """
        try:
            # ã‚¿ã‚¤ãƒˆãƒ«æŠ½å‡º
            title = self._extract_job_title(soup)
            if not title:
                return None
            
            # èª¬æ˜æ–‡æŠ½å‡º
            description = self._extract_job_description(soup)
            
            # å‹¤å‹™åœ°æŠ½å‡º
            location = self._extract_job_location(soup)
            
            # è·ç¨®ãƒ»ã‚«ãƒ†ã‚´ãƒªæŠ½å‡º
            category = self._extract_job_category(soup)
            
            return {
                'title': title,
                'url': url,
                'job_code': job_code,
                'description': description or "è©³ç´°æƒ…å ±ãªã—",
                'location': location or "å‹¤å‹™åœ°ä¸æ˜",
                'category': category or "ã‚«ãƒ†ã‚´ãƒªä¸æ˜",
                'source': 'jposting_complete'
            }
            
        except Exception as e:
            print(f"      âŒ æ±‚äººè©³ç´°è§£æã‚¨ãƒ©ãƒ¼: {e}")
            return None
    
    def _extract_job_title(self, soup: BeautifulSoup) -> str:
        """æ±‚äººã‚¿ã‚¤ãƒˆãƒ«ã‚’æŠ½å‡º"""
        # h2ã‚¿ã‚°ã‹ã‚‰æŠ½å‡ºï¼ˆæœ€ã‚‚å¯èƒ½æ€§ãŒé«˜ã„ï¼‰
        h2_tags = soup.find_all('h2')
        for h2 in h2_tags:
            text = h2.get_text().strip()
            if text and len(text) > 5 and self._looks_like_job_title(text):
                return self._clean_title(text)
        
        # h1ã‚¿ã‚°ã‹ã‚‰æŠ½å‡º
        h1_tags = soup.find_all('h1')
        for h1 in h1_tags:
            text = h1.get_text().strip()
            if text and len(text) > 5 and text != 'æ±‚äººè©³ç´°':
                return self._clean_title(text)
        
        # ãƒ†ãƒ¼ãƒ–ãƒ«ã®æœ€åˆã®è¡Œã‹ã‚‰æŠ½å‡ºã‚’è©¦è¡Œ
        tables = soup.find_all('table')
        for table in tables:
            rows = table.find_all('tr')
            for row in rows[:3]:  # æœ€åˆã®3è¡Œã‚’ãƒã‚§ãƒƒã‚¯
                cells = row.find_all(['td', 'th'])
                if len(cells) >= 2:
                    # 2åˆ—ç›®ï¼ˆè·ç¨®ååˆ—ï¼‰ã‚’ãƒã‚§ãƒƒã‚¯
                    title_text = cells[1].get_text().strip()
                    if self._looks_like_job_title(title_text):
                        return self._clean_title(title_text)
        
        # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯: ãƒšãƒ¼ã‚¸å†…ã®é•·ã‚ã®ãƒ†ã‚­ã‚¹ãƒˆã‹ã‚‰æ¨æ¸¬
        all_text = soup.get_text()
        lines = [line.strip() for line in all_text.split('\n') if line.strip()]
        
        for line in lines[:30]:  # æœ€åˆã®30è¡Œã‹ã‚‰æ¢ã™
            if self._looks_like_job_title(line) and len(line) > 10:
                return self._clean_title(line)
        
        # æœ€çµ‚ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯: job_codeã‹ã‚‰ç”Ÿæˆ
        return f"æ±‚äººç•ªå·_{soup.find('input', {'name': 'job_code'}) or 'unknown'}"
    
    def _extract_job_description(self, soup: BeautifulSoup) -> str:
        """æ±‚äººèª¬æ˜ã‚’æŠ½å‡º"""
        # èª¬æ˜ãŒå«ã¾ã‚Œã¦ã„ãã†ãªè¦ç´ ã‚’æ¢ã™
        desc_selectors = [
            '.job-description',
            '.description', 
            '.content',
            '.detail',
            '[class*="desc"]',
            'p'
        ]
        
        descriptions = []
        for selector in desc_selectors:
            elements = soup.select(selector)
            for element in elements:
                text = element.get_text().strip()
                if len(text) > 50 and len(text) < 1000:  # é©åˆ‡ãªé•·ã•
                    descriptions.append(text)
        
        if descriptions:
            # æœ€ã‚‚è©³ç´°ãªèª¬æ˜ã‚’é¸æŠ
            return max(descriptions, key=len)[:500]  # 500æ–‡å­—ã¾ã§
        
        return None
    
    def _extract_job_location(self, soup: BeautifulSoup) -> str:
        """å‹¤å‹™åœ°ã‚’æŠ½å‡º"""
        location_keywords = ['å‹¤å‹™åœ°', 'æ‰€åœ¨åœ°', 'å‹¤å‹™å…ˆ', 'å ´æ‰€', 'æ±äº¬', 'å¤§é˜ª', 'ç¥æˆ¸', 'è£½ä½œæ‰€']
        
        all_text = soup.get_text()
        lines = all_text.split('\\n')
        
        for line in lines:
            line = line.strip()
            if any(keyword in line for keyword in location_keywords):
                if len(line) < 100:  # é•·ã™ããªã„
                    return line
        
        return None
    
    def _extract_job_category(self, soup: BeautifulSoup) -> str:
        """è·ç¨®ã‚«ãƒ†ã‚´ãƒªã‚’æŠ½å‡º"""
        category_keywords = {
            'ã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢': 'ã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢ç³»',
            'é–‹ç™º': 'ã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢ç³»', 
            'è¨­è¨ˆ': 'ã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢ç³»',
            'å–¶æ¥­': 'ãƒ“ã‚¸ãƒã‚¹ç³»',
            'ä¼ç”»': 'ãƒ“ã‚¸ãƒã‚¹ç³»',
            'äººäº‹': 'ãƒ“ã‚¸ãƒã‚¹ç³»',
            'ãƒãƒ¼ã‚±ãƒ†ã‚£ãƒ³ã‚°': 'ãƒ“ã‚¸ãƒã‚¹ç³»',
            'è£½é€ ': 'è£½é€ ç³»',
            'å“è³ª': 'è£½é€ ç³»',
            'ç”Ÿç”£': 'è£½é€ ç³»'
        }
        
        all_text = soup.get_text().lower()
        
        for keyword, category in category_keywords.items():
            if keyword.lower() in all_text:
                return category
        
        return None
    
    def _looks_like_job_title(self, text: str) -> bool:
        """ãƒ†ã‚­ã‚¹ãƒˆãŒæ±‚äººã‚¿ã‚¤ãƒˆãƒ«ã‚‰ã—ã„ã‹ãƒã‚§ãƒƒã‚¯"""
        if not text or len(text) < 10 or len(text) > 200:
            return False
        
        job_indicators = [
            'ã€', 'ã€‘', 'ã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢', 'æŠ€è¡“è€…', 'é–‹ç™º', 'è¨­è¨ˆ', 'å–¶æ¥­', 
            'ä¼ç”»', 'ç®¡ç†', 'ã‚·ã‚¹ãƒ†ãƒ ', 'ã‚½ãƒ•ãƒˆ', 'è£½é€ ', 'ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆ',
            'WEBé¢æ¥', 'æ±äº¬', 'å¤§é˜ª', 'ç¥æˆ¸', 'è£½ä½œæ‰€', 'æ‹…å½“', 'ä¸»ä»»'
        ]
        
        return any(indicator in text for indicator in job_indicators)
    
    def _clean_title(self, title: str) -> str:
        """æ±‚äººã‚¿ã‚¤ãƒˆãƒ«ã‚’ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—"""
        if not title:
            return ""
        
        # ä¸è¦ãªæ–‡å­—ã‚’é™¤å»
        title = re.sub(r'\\s+', ' ', title)
        title = re.sub(r'\\n+', ' ', title) 
        title = title.strip()
        
        # é•·ã™ãã‚‹å ´åˆã¯åˆ‡ã‚Šè©°ã‚
        if len(title) > 150:
            title = title[:147] + "..."
        
        return title
