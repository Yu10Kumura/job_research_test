"""
å¼·åŒ–ç‰ˆJPOSTINGæ±‚äººæŠ½å‡ºå™¨ - éšå±¤æ¢ç´¢æ©Ÿèƒ½ä»˜ã
ã™ã¹ã¦ã®æ±‚äººã‚’éšå±¤çš„ã«æ¢ç´¢ã—ã¦å–å¾—
"""

import requests
from bs4 import BeautifulSoup
import re
from typing import List, Dict, Set
import time
from urllib.parse import urljoin, urlparse
import logging

class EnhancedJPOSTINGExtractor:
    def __init__(self, max_jobs: int = 50, max_depth: int = 3):
        self.base_url = "https://progres02.jposting.net"
        self.max_jobs = max_jobs
        self.max_depth = max_depth
        self.session = requests.Session()
        self.session.headers.update({
            'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
        })
        self.visited_urls: Set[str] = set()
        self.found_jobs: List[Dict[str, str]] = []
        
    def extract_all_jobs(self, start_url: str) -> List[Dict[str, str]]:
        """
        é–‹å§‹URLã‹ã‚‰éšå±¤çš„ã«ã™ã¹ã¦ã®æ±‚äººã‚’æ¢ç´¢ãƒ»æŠ½å‡º
        """
        print(f"ğŸš€ éšå±¤æ¢ç´¢é–‹å§‹: {start_url}")
        print(f"ğŸ“‹ æœ€å¤§æ±‚äººæ•°: {self.max_jobs}ä»¶, æœ€å¤§éšå±¤: {self.max_depth}")
        
        self.found_jobs = []
        self.visited_urls = set()
        
        # éšå±¤æ¢ç´¢å®Ÿè¡Œ
        self._explore_recursive(start_url, depth=0)
        
        print(f"âœ… æ¢ç´¢å®Œäº†: {len(self.found_jobs)}ä»¶ã®æ±‚äººã‚’ç™ºè¦‹")
        return self.found_jobs
    
    def _explore_recursive(self, url: str, depth: int = 0):
        """
        å†å¸°çš„ã«éšå±¤æ¢ç´¢
        """
        # åˆ¶é™ãƒã‚§ãƒƒã‚¯
        if depth > self.max_depth or len(self.found_jobs) >= self.max_jobs:
            return
        
        # é‡è¤‡ãƒã‚§ãƒƒã‚¯
        if url in self.visited_urls:
            return
            
        self.visited_urls.add(url)
        
        try:
            print(f"{'  ' * depth}ğŸ” æ¢ç´¢ä¸­ (éšå±¤{depth}): {url}")
            response = self.session.get(url, timeout=15)
            response.raise_for_status()
            response.encoding = 'utf-8'
            
            soup = BeautifulSoup(response.text, 'html.parser')
            
            # ç¾åœ¨ã®ãƒšãƒ¼ã‚¸ã‹ã‚‰æ±‚äººã‚’æŠ½å‡º
            current_jobs = self._extract_jobs_from_page(soup, url, depth)
            self.found_jobs.extend(current_jobs)
            
            # åˆ¶é™ã«é”ã—ãŸå ´åˆã¯åœæ­¢
            if len(self.found_jobs) >= self.max_jobs:
                print(f"{'  ' * depth}âœ‹ æœ€å¤§æ±‚äººæ•°ã«é”ã—ãŸãŸã‚åœæ­¢")
                return
            
            # æ¬¡ã®éšå±¤ã®URLã‚’æ¢ç´¢
            if depth < self.max_depth:
                next_urls = self._find_job_related_links(soup, url)
                print(f"{'  ' * depth}ğŸ”— æ¬¡éšå±¤ã®ãƒªãƒ³ã‚¯: {len(next_urls)}ä»¶")
                
                for next_url in next_urls[:10]:  # å„éšå±¤ã§æœ€å¤§10ãƒªãƒ³ã‚¯ã¾ã§
                    if len(self.found_jobs) >= self.max_jobs:
                        break
                    time.sleep(0.5)  # ãƒ¬ãƒ¼ãƒˆåˆ¶é™
                    self._explore_recursive(next_url, depth + 1)
                    
        except requests.exceptions.RequestException as e:
            print(f"{'  ' * depth}âŒ ã‚¢ã‚¯ã‚»ã‚¹ã‚¨ãƒ©ãƒ¼: {e}")
        except Exception as e:
            print(f"{'  ' * depth}âŒ è§£æã‚¨ãƒ©ãƒ¼: {e}")
    
    def _extract_jobs_from_page(self, soup: BeautifulSoup, page_url: str, depth: int) -> List[Dict[str, str]]:
        """
        å˜ä¸€ãƒšãƒ¼ã‚¸ã‹ã‚‰æ±‚äººæƒ…å ±ã‚’æŠ½å‡º
        """
        jobs = []
        
        # ãƒ‘ã‚¿ãƒ¼ãƒ³1: ãƒ†ãƒ¼ãƒ–ãƒ«å½¢å¼ã®æ±‚äºº
        tables = soup.find_all('table')
        for table in tables:
            jobs.extend(self._parse_table_jobs(table, page_url))
        
        # ãƒ‘ã‚¿ãƒ¼ãƒ³2: ãƒªã‚¹ãƒˆå½¢å¼ã®æ±‚äºº
        for tag in ['ul', 'ol']:
            lists = soup.find_all(tag)
            for ul in lists:
                jobs.extend(self._parse_list_jobs(ul, page_url))
        
        # ãƒ‘ã‚¿ãƒ¼ãƒ³3: divè¦ç´ ã®æ±‚äºº
        divs = soup.find_all('div', class_=re.compile(r'(job|position|career|recruit)', re.I))
        for div in divs:
            job_info = self._parse_div_job(div, page_url)
            if job_info:
                jobs.append(job_info)
        
        # ãƒ‘ã‚¿ãƒ¼ãƒ³4: ä¸€èˆ¬çš„ãªdivè¦ç´ ã‹ã‚‰æ±‚äººã‚‰ã—ã„ãƒ†ã‚­ã‚¹ãƒˆã‚’æ¢ç´¢
        all_divs = soup.find_all('div')
        for div in all_divs:
            text = div.get_text().strip()
            if self._looks_like_job_title(text):
                job_info = self._create_job_info(text, page_url, div)
                if job_info:
                    jobs.append(job_info)
        
        # é‡è¤‡é™¤å»
        unique_jobs = []
        seen_titles = set()
        
        for job in jobs:
            if job['title'] not in seen_titles:
                seen_titles.add(job['title'])
                unique_jobs.append(job)
                if len(unique_jobs) <= 5:  # ãƒ­ã‚°è¡¨ç¤ºåˆ¶é™
                    print(f"{'  ' * depth}  ğŸ“„ {job['title'][:60]}...")
        
        print(f"{'  ' * depth}ğŸ“Š ã“ã®ãƒšãƒ¼ã‚¸ã‹ã‚‰ {len(unique_jobs)}ä»¶ã®æ±‚äººã‚’æŠ½å‡º")
        return unique_jobs
    
    def _parse_table_jobs(self, table, page_url: str) -> List[Dict[str, str]]:
        """ãƒ†ãƒ¼ãƒ–ãƒ«ã‹ã‚‰æ±‚äººæƒ…å ±ã‚’æŠ½å‡º"""
        jobs = []
        rows = table.find_all('tr')
        
        for row in rows:
            cells = row.find_all(['td', 'th'])
            if len(cells) >= 1:
                # æœ€åˆã®ã‚»ãƒ«ã‚’æ±‚äººã‚¿ã‚¤ãƒˆãƒ«ã¨ã—ã¦æ‰±ã†
                title_text = cells[0].get_text().strip()
                if self._looks_like_job_title(title_text):
                    # URLã‚’æ¢ã™
                    link = cells[0].find('a')
                    job_url = urljoin(page_url, link['href']) if link and link.get('href') else page_url
                    
                    jobs.append({
                        'title': self._clean_title(title_text),
                        'url': job_url,
                        'source': 'table'
                    })
        
        return jobs
    
    def _parse_list_jobs(self, ul, page_url: str) -> List[Dict[str, str]]:
        """ãƒªã‚¹ãƒˆã‹ã‚‰æ±‚äººæƒ…å ±ã‚’æŠ½å‡º"""
        jobs = []
        items = ul.find_all('li')
        
        for item in items:
            text = item.get_text().strip()
            if self._looks_like_job_title(text):
                link = item.find('a')
                job_url = urljoin(page_url, link['href']) if link and link.get('href') else page_url
                
                jobs.append({
                    'title': self._clean_title(text),
                    'url': job_url,
                    'source': 'list'
                })
        
        return jobs
    
    def _parse_div_job(self, div, page_url: str) -> Dict[str, str]:
        """divè¦ç´ ã‹ã‚‰æ±‚äººæƒ…å ±ã‚’æŠ½å‡º"""
        text = div.get_text().strip()
        if not self._looks_like_job_title(text):
            return None
        
        link = div.find('a')
        job_url = urljoin(page_url, link['href']) if link and link.get('href') else page_url
        
        return {
            'title': self._clean_title(text),
            'url': job_url,
            'source': 'div'
        }
    
    def _create_job_info(self, text: str, page_url: str, element) -> Dict[str, str]:
        """æ±ç”¨æ±‚äººæƒ…å ±ä½œæˆ"""
        if not self._looks_like_job_title(text):
            return None
        
        link = element.find('a') if element else None
        job_url = urljoin(page_url, link['href']) if link and link.get('href') else page_url
        
        return {
            'title': self._clean_title(text),
            'url': job_url,
            'source': 'general'
        }
    
    def _find_job_related_links(self, soup: BeautifulSoup, current_url: str) -> List[str]:
        """æ±‚äººé–¢é€£ã®ãƒªãƒ³ã‚¯ã‚’æ¢ç´¢"""
        links = set()
        
        # ã™ã¹ã¦ã®ãƒªãƒ³ã‚¯ã‚’å–å¾—
        for link in soup.find_all('a', href=True):
            href = link['href']
            full_url = urljoin(current_url, href)
            
            # æ±‚äººé–¢é€£ã®URLã‹ãƒã‚§ãƒƒã‚¯
            if self._is_job_related_url(full_url, link.get_text().strip()):
                links.add(full_url)
        
        # ç¾åœ¨ã®ãƒ‰ãƒ¡ã‚¤ãƒ³å†…ã®ãƒªãƒ³ã‚¯ã®ã¿
        domain_links = []
        current_domain = urlparse(current_url).netloc
        
        for link in links:
            if urlparse(link).netloc == current_domain:
                domain_links.append(link)
        
        return list(domain_links)[:20]  # æœ€å¤§20ãƒªãƒ³ã‚¯
    
    def _is_job_related_url(self, url: str, link_text: str) -> bool:
        """URLãŒæ±‚äººé–¢é€£ã‹ã©ã†ã‹åˆ¤å®š"""
        job_keywords = [
            'job', 'career', 'recruit', 'position', 'employment',
            'æ±‚äºº', 'å‹Ÿé›†', 'æ¡ç”¨', 'è·ç¨®', 'ã‚­ãƒ£ãƒªã‚¢'
        ]
        
        url_lower = url.lower()
        text_lower = link_text.lower()
        
        return any(keyword in url_lower or keyword in text_lower 
                  for keyword in job_keywords)
    
    def _looks_like_job_title(self, text: str) -> bool:
        """ãƒ†ã‚­ã‚¹ãƒˆãŒæ±‚äººã‚¿ã‚¤ãƒˆãƒ«ã‚‰ã—ã„ã‹ãƒã‚§ãƒƒã‚¯"""
        if not text or len(text) < 5 or len(text) > 200:
            return False
        
        # æ±‚äººã‚¿ã‚¤ãƒˆãƒ«ã«ã‚ˆãå«ã¾ã‚Œã‚‹ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰
        job_indicators = [
            'ã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢', 'æŠ€è¡“è€…', 'é–‹ç™º', 'è¨­è¨ˆ', 'å–¶æ¥­', 'ä¼ç”»', 'ç®¡ç†',
            'ã€', 'ã€‘', 'ã‚·ã‚¹ãƒ†ãƒ ', 'ã‚½ãƒ•ãƒˆ', 'ãƒãƒ¼ãƒ‰', 'è£½é€ ', 'å“è³ª',
            'ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆ', 'ãƒãƒãƒ¼ã‚¸ãƒ£ãƒ¼', 'ä¸»ä»»', 'èª²é•·', 'éƒ¨é•·', 'æ‹…å½“',
            'æ­£ç¤¾å“¡', 'å¥‘ç´„', 'æ´¾é£', 'WEBé¢æ¥', 'æ±äº¬', 'å¤§é˜ª', 'ç¥æˆ¸'
        ]
        
        return any(indicator in text for indicator in job_indicators)
    
    def _clean_title(self, title: str) -> str:
        """æ±‚äººã‚¿ã‚¤ãƒˆãƒ«ã‚’ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—"""
        if not title:
            return ""
        
        # ä¸è¦ãªæ–‡å­—ã‚’é™¤å»
        title = re.sub(r'\s+', ' ', title)  # è¤‡æ•°ç©ºç™½ã‚’1ã¤ã«
        title = re.sub(r'\n+', ' ', title)  # æ”¹è¡Œã‚’ç©ºç™½ã«
        title = title.strip()
        
        # é•·ã™ãã‚‹å ´åˆã¯åˆ‡ã‚Šè©°ã‚
        if len(title) > 150:
            title = title[:147] + "..."
        
        return title
