"""
JPOSTINGæ±‚äººæŠ½å‡ºå™¨ - ä¸‰è±é›»æ©Ÿç”¨
https://progres02.jposting.net/pgmitsubishielectric/u/job.phtml ã‹ã‚‰æ±‚äººæƒ…å ±ã‚’æŠ½å‡º
"""

import requests
from bs4 import BeautifulSoup
import re
from typing import List, Dict
import time

class JPOSTINGExtractor:
    def __init__(self):
        self.base_url = "https://progres02.jposting.net"
        self.session = requests.Session()
        self.session.headers.update({
            'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
        })
    
    def extract_jobs(self, url: str) -> List[Dict[str, str]]:
        """
        æ±‚äººä¸€è¦§URLã‹ã‚‰æ±‚äººæƒ…å ±ã‚’æŠ½å‡º
        """
        try:
            print(f"ğŸ” æ±‚äººãƒšãƒ¼ã‚¸ã«ã‚¢ã‚¯ã‚»ã‚¹ä¸­: {url}")
            response = self.session.get(url, timeout=10)
            response.raise_for_status()
            
            # æ–‡å­—ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°ã‚’æ˜ç¤ºçš„ã«è¨­å®š
            response.encoding = 'utf-8'
            
            soup = BeautifulSoup(response.text, 'html.parser')
            jobs = []
            
            # æ±‚äººæƒ…å ±ã‚’æ¢ã™ - è·ç¨®åã®ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’æ¢ç´¢
            # ã‚µã‚¤ãƒˆæ§‹é€ ã‹ã‚‰ã€è·ç¨®åã¯æ§˜ã€…ãªå½¢å¼ã§è¡¨ç¤ºã•ã‚Œã¦ã„ã‚‹
            job_elements = self._find_job_elements(soup)
            
            print(f"ğŸ“‹ {len(job_elements)}ä»¶ã®æ±‚äººã‚’ç™ºè¦‹")
            
            for i, element in enumerate(job_elements):
                try:
                    job_info = self._parse_job_element(element)
                    if job_info:
                        jobs.append(job_info)
                        print(f"  {i+1}. {job_info['title'][:50]}...")
                        
                        # æœ€å¤§5ä»¶ã¾ã§
                        if len(jobs) >= 5:
                            print("  âš ï¸ 5ä»¶ã«é”ã—ãŸãŸã‚æŠ½å‡ºã‚’åœæ­¢")
                            break
                            
                except Exception as e:
                    print(f"  âš ï¸ æ±‚äºº {i+1} ã®è§£æã‚¨ãƒ©ãƒ¼: {e}")
                    continue
            
            return jobs
            
        except requests.exceptions.RequestException as e:
            raise Exception(f"ã‚µã‚¤ãƒˆã¸ã®ã‚¢ã‚¯ã‚»ã‚¹ã‚¨ãƒ©ãƒ¼: {e}")
        except Exception as e:
            raise Exception(f"æ±‚äººæŠ½å‡ºã‚¨ãƒ©ãƒ¼: {e}")
    
    def _find_job_elements(self, soup: BeautifulSoup) -> List:
        """
        æ±‚äººè¦ç´ ã‚’è¦‹ã¤ã‘ã‚‹
        """
        job_elements = []
        
        # ãƒ‘ã‚¿ãƒ¼ãƒ³1: ãƒ†ãƒ¼ãƒ–ãƒ«å½¢å¼ã®æ±‚äººæƒ…å ±
        tables = soup.find_all('table')
        for table in tables:
            rows = table.find_all('tr')
            for row in rows:
                cells = row.find_all(['td', 'th'])
                if len(cells) >= 2:  # è·ç¨®åã¨å‹¤å‹™åœ°ãŒã‚ã‚‹è¡Œ
                    job_elements.append(row)
        
        # ãƒ‘ã‚¿ãƒ¼ãƒ³2: ãƒªã‚¹ãƒˆå½¢å¼
        lists = soup.find_all(['ul', 'ol'])
        for ul in lists:
            items = ul.find_all('li')
            for item in items:
                if item.get_text().strip():  # ç©ºã§ãªã„ã‚¢ã‚¤ãƒ†ãƒ 
                    job_elements.append(item)
        
        # ãƒ‘ã‚¿ãƒ¼ãƒ³3: divè¦ç´ ã§æ§‹æˆã•ã‚Œã¦ã„ã‚‹å ´åˆ
        divs = soup.find_all('div')
        for div in divs:
            # æ±‚äººã‚¿ã‚¤ãƒˆãƒ«ã‚‰ã—ã„ãƒ†ã‚­ã‚¹ãƒˆãŒå«ã¾ã‚Œã¦ã„ã‚‹div
            text = div.get_text().strip()
            if any(keyword in text for keyword in ['ã€', 'ã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢', 'æŠ€è¡“', 'å–¶æ¥­', 'ç®¡ç†']):
                if len(text) > 10 and len(text) < 200:  # é©åˆ‡ãªé•·ã•
                    job_elements.append(div)
        
        # ãƒ‘ã‚¿ãƒ¼ãƒ³4: ç›´æ¥ãƒ†ã‚­ã‚¹ãƒˆã‚’ã‚¹ã‚­ãƒ£ãƒ³
        all_text = soup.get_text()
        lines = all_text.split('\n')
        for line in lines:
            line = line.strip()
            if self._looks_like_job_title(line):
                # ç–‘ä¼¼è¦ç´ ã¨ã—ã¦ä½œæˆ
                fake_element = type('Element', (), {
                    'get_text': lambda: line,
                    'find': lambda *args: None,
                    'find_all': lambda *args: []
                })()
                job_elements.append(fake_element)
        
        # é‡è¤‡é™¤å»ã¨çµã‚Šè¾¼ã¿
        unique_jobs = []
        seen_titles = set()
        
        for element in job_elements:
            text = element.get_text().strip()
            if text and text not in seen_titles and self._looks_like_job_title(text):
                seen_titles.add(text)
                unique_jobs.append(element)
                if len(unique_jobs) >= 10:  # æœ€å¤§10ä»¶ã‚’å€™è£œã¨ã™ã‚‹
                    break
        
        return unique_jobs
    
    def _looks_like_job_title(self, text: str) -> bool:
        """
        ãƒ†ã‚­ã‚¹ãƒˆãŒæ±‚äººã‚¿ã‚¤ãƒˆãƒ«ã£ã½ã„ã‹ã‚’åˆ¤å®š
        """
        if not text or len(text) < 5 or len(text) > 150:
            return False
        
        # æ±‚äººã‚¿ã‚¤ãƒˆãƒ«ã®ç‰¹å¾´
        job_indicators = [
            'ã€', 'ã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢', 'æŠ€è¡“è€…', 'è¨­è¨ˆ', 'é–‹ç™º', 'ç®¡ç†', 'å–¶æ¥­',
            'æ‹…å½“', 'ã‚¹ã‚¿ãƒƒãƒ•', 'è·', 'æ¥­å‹™', 'è²¬ä»»è€…', 'ãƒãƒãƒ¼ã‚¸ãƒ£ãƒ¼',
            'è£½ä½œæ‰€', 'äº‹æ¥­æ‰€', 'NEW', 'çµŒé¨“', 'æœªçµŒé¨“', 'WEBé¢æ¥'
        ]
        
        # é™¤å¤–ãƒ‘ã‚¿ãƒ¼ãƒ³
        exclude_patterns = [
            'è·ç¨®å', 'å‹¤å‹™åœ°', 'å¿œå‹Ÿ', 'é¸è€ƒ', 'é¢æ¥', 'èª¬æ˜ä¼š',
            'å‹•ç”»', 'YouTube', 'http', 'www', 'Copyright', 'Â©',
            'å…¨å›½', 'åŒ—æµ·é“', 'æ±äº¬', 'å¤§é˜ª', 'ç¥å¥ˆå·'  # å˜ç‹¬ã®åœ°åã¯é™¤å¤–
        ]
        
        # é™¤å¤–ãƒ‘ã‚¿ãƒ¼ãƒ³ã«ãƒãƒƒãƒã™ã‚‹ã‹ãƒã‚§ãƒƒã‚¯
        for exclude in exclude_patterns:
            if exclude in text and len(text.replace(exclude, '').strip()) < 5:
                return False
        
        # æ±‚äººæŒ‡æ¨™ãŒå«ã¾ã‚Œã¦ã„ã‚‹ã‹ãƒã‚§ãƒƒã‚¯
        return any(indicator in text for indicator in job_indicators)
    
    def _parse_job_element(self, element) -> Dict[str, str]:
        """
        æ±‚äººè¦ç´ ã‹ã‚‰æƒ…å ±ã‚’æŠ½å‡º
        """
        # ã‚¿ã‚¤ãƒˆãƒ«å–å¾—
        title = self._extract_title(element)
        if not title:
            return None
        
        # URLå–å¾—ã®è©¦è¡Œ
        job_url = self._extract_url(element)
        
        return {
            'title': title,
            'url': job_url
        }
    
    def _extract_title(self, element) -> str:
        """
        æ±‚äººã‚¿ã‚¤ãƒˆãƒ«ã‚’æŠ½å‡º
        """
        # è¦ç´ ã‹ã‚‰ãƒ†ã‚­ã‚¹ãƒˆã‚’å–å¾—
        try:
            title = element.get_text().strip()
        except:
            return None
        
        # æ–‡å­—åŒ–ã‘å¯¾ç­– - ãƒ‡ã‚³ãƒ¼ãƒ‰ã®è©¦è¡Œ
        if title and ('ï¿½' in title or len(title.encode('utf-8', errors='ignore')) < len(title) * 0.5):
            # æ–‡å­—åŒ–ã‘ã—ã¦ã„ã‚‹å¯èƒ½æ€§ãŒé«˜ã„å ´åˆã¯ã‚¹ã‚­ãƒƒãƒ—
            return None
        
        # ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—
        title = re.sub(r'\s+', ' ', title)  # è¤‡æ•°ã®ç©ºç™½ã‚’1ã¤ã«
        title = re.sub(r'\n+', ' ', title)  # æ”¹è¡Œã‚’ç©ºç™½ã«
        
        # é•·ã™ãã‚‹å ´åˆã¯æœ€åˆã®éƒ¨åˆ†ã®ã¿ã‚’ä½¿ç”¨
        lines = title.split(' ')
        if len(title) > 100 and len(lines) > 3:
            title = ' '.join(lines[:3]) + "..."
        elif len(title) > 100:
            title = title[:97] + "..."
        
        return title if title and len(title) > 5 else None
    
    def _extract_url(self, element) -> str:
        """
        æ±‚äººè©³ç´°URLã‚’æŠ½å‡ºï¼ˆè©¦è¡Œï¼‰
        """
        # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆURLï¼ˆè©³ç´°ãƒšãƒ¼ã‚¸ãŒè¦‹ã¤ã‹ã‚‰ãªã„å ´åˆï¼‰
        default_url = "https://progres02.jposting.net/pgmitsubishielectric/u/job.phtml"
        
        try:
            # ãƒªãƒ³ã‚¯è¦ç´ ã‚’æ¢ã™
            link = element.find('a')
            if link and link.get('href'):
                href = link.get('href')
                if href.startswith('http'):
                    return href
                elif href.startswith('/'):
                    return self.base_url + href
                else:
                    return self.base_url + '/' + href
            
            # è¦ªè¦ç´ ã«ãƒªãƒ³ã‚¯ãŒã‚ã‚‹ã‹ãƒã‚§ãƒƒã‚¯
            parent = getattr(element, 'parent', None)
            if parent:
                parent_link = parent.find('a')
                if parent_link and parent_link.get('href'):
                    href = parent_link.get('href')
                    if href.startswith('http'):
                        return href
                    elif href.startswith('/'):
                        return self.base_url + href
            
            # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆURLã‚’è¿”ã™
            return default_url
            
        except Exception:
            return default_url

def extract_jobs_from_url(url: str) -> List[Dict[str, str]]:
    """
    å¤–éƒ¨ã‚¤ãƒ³ã‚¿ãƒ¼ãƒ•ã‚§ãƒ¼ã‚¹: URLã‹ã‚‰æ±‚äººæƒ…å ±ã‚’æŠ½å‡º
    """
    extractor = JPOSTINGExtractor()
    return extractor.extract_jobs(url)

if __name__ == "__main__":
    # ãƒ†ã‚¹ãƒˆå®Ÿè¡Œ
    url = "https://progres02.jposting.net/pgmitsubishielectric/u/job.phtml"
    
    try:
        print("ğŸš€ JPOSTINGæ±‚äººæŠ½å‡ºå™¨ - ãƒ†ã‚¹ãƒˆå®Ÿè¡Œ")
        print(f"å¯¾è±¡URL: {url}")
        print("-" * 60)
        
        jobs = extract_jobs_from_url(url)
        
        print(f"\nâœ… æŠ½å‡ºå®Œäº†: {len(jobs)}ä»¶ã®æ±‚äºº")
        print("=" * 60)
        
        for i, job in enumerate(jobs, 1):
            print(f"\n{i}. {job['title']}")
            print(f"   URL: {job['url']}")
        
        print("\n" + "=" * 60)
        print("ğŸ‰ ãƒ†ã‚¹ãƒˆå®Œäº†")
        
    except Exception as e:
        print(f"âŒ ã‚¨ãƒ©ãƒ¼: {e}")
