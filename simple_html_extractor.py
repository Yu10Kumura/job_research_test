"""
ã‚·ãƒ³ãƒ—ãƒ«æ±‚äººæŠ½å‡ºå™¨ - HTMLã‹ã‚‰ç›´æ¥å…¨æ±‚äººã‚’å–å¾—
"""

import requests
from bs4 import BeautifulSoup
import re
from typing import List, Dict

class SimpleJobExtractor:
    def __init__(self):
        self.session = requests.Session()
        self.session.headers.update({
            'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
        })
    
    def extract_all_jobs(self, url: str) -> List[Dict[str, str]]:
        """
        HTMLã‹ã‚‰ç›´æ¥å…¨æ±‚äººã‚’æŠ½å‡º
        """
        print(f"ğŸ” æ±‚äººãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—ä¸­: {url}")
        
        try:
            response = self.session.get(url, timeout=15)
            
            # è¤‡æ•°ã®ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°ã‚’è©¦ã™
            encodings = ['euc-jp', 'shift_jis', 'utf-8']
            soup = None
            
            for encoding in encodings:
                try:
                    response.encoding = encoding
                    soup = BeautifulSoup(response.text, 'html.parser')
                    test_text = soup.get_text()[:1000]
                    
                    # æ–‡å­—åŒ–ã‘ãƒã‚§ãƒƒã‚¯ - æ—¥æœ¬èªã®æ–‡å­—ãŒæ­£ã—ãè¡¨ç¤ºã•ã‚Œã¦ã„ã‚‹ã‹
                    if 'ï¿½' not in test_text and any(char in test_text for char in 'ã‚ã„ã†ãˆãŠã‹ããã‘ã“'):
                        print(f"âœ… ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚° {encoding} ã§èª­ã¿è¾¼ã¿æˆåŠŸ")
                        break
                    
                except:
                    continue
            
            if not soup:
                print("âš ï¸ é©åˆ‡ãªã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°ãŒè¦‹ã¤ã‹ã‚‰ãªã„ãŸã‚ã€UTF-8ã§å‡¦ç†ã‚’ç¶šè¡Œ")
                response.encoding = 'utf-8'
                soup = BeautifulSoup(response.text, 'html.parser')
            
            # æ±‚äººãƒªãƒ³ã‚¯ã‹ã‚‰æƒ…å ±ã‚’æŠ½å‡º
            jobs = self._extract_jobs_from_links(soup, url)
            
            print(f"ğŸ“Š æŠ½å‡ºå®Œäº†: {len(jobs)}ä»¶ã®æ±‚äººã‚’å–å¾—")
            return jobs
            
        except Exception as e:
            print(f"âŒ ã‚¨ãƒ©ãƒ¼: {e}")
            return []
    
    def _extract_jobs_from_links(self, soup: BeautifulSoup, base_url: str) -> List[Dict[str, str]]:
        """
        æ±‚äººãƒªãƒ³ã‚¯ã‹ã‚‰æƒ…å ±ã‚’æŠ½å‡º
        """
        jobs = []
        
        # job_codeã‚’å«ã‚€ãƒªãƒ³ã‚¯ã‚’å…¨ã¦å–å¾—
        links = soup.find_all('a', href=True)
        
        for link in links:
            href = link['href']
            
            # job_codeãŒå«ã¾ã‚Œã¦ã„ã‚‹ãƒªãƒ³ã‚¯ã®ã¿å‡¦ç†
            if 'job_code=' in href:
                try:
                    # job_codeã‚’æŠ½å‡º
                    job_code_match = re.search(r'job_code=(\d+)', href)
                    if job_code_match:
                        job_code = job_code_match.group(1)
                        
                        # ãƒªãƒ³ã‚¯ã®ãƒ†ã‚­ã‚¹ãƒˆã‚’å–å¾—ï¼ˆã“ã‚ŒãŒæ±‚äººã‚¿ã‚¤ãƒˆãƒ«ï¼‰
                        title = link.get_text().strip()
                        
                        # ç©ºã§ãªã„ã€é©åˆ‡ãªé•·ã•ã®ã‚¿ã‚¤ãƒˆãƒ«ã®ã¿æ¡ç”¨
                        if title and len(title) > 5 and len(title) < 300:
                            # å®Œå…¨ãªURLã‚’æ§‹ç¯‰
                            if href.startswith('http'):
                                job_url = href
                            else:
                                job_url = f"https://progres02.jposting.net/pgmitsubishielectric/u/{href}"
                            
                            # åŸºæœ¬çš„ãªæƒ…å ±ã‚’å«ã‚€æ±‚äººã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã‚’ä½œæˆ
                            job_info = {
                                'title': self._clean_title(title),
                                'url': job_url,
                                'job_code': job_code,
                                'source': 'direct_html'
                            }
                            
                            # é‡è¤‡ãƒã‚§ãƒƒã‚¯
                            if not any(job['job_code'] == job_code for job in jobs):
                                jobs.append(job_info)
                                
                                # é€²æ—è¡¨ç¤ºï¼ˆæœ€åˆã®10ä»¶ï¼‰
                                if len(jobs) <= 10:
                                    print(f"  {len(jobs):3d}. {title[:70]}...")
                
                except Exception as e:
                    print(f"  âš ï¸ ãƒªãƒ³ã‚¯è§£æã‚¨ãƒ©ãƒ¼: {e}")
                    continue
        
        return jobs
    
    def _clean_title(self, title: str) -> str:
        """
        æ±‚äººã‚¿ã‚¤ãƒˆãƒ«ã‚’ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—
        """
        if not title:
            return ""
        
        # è¤‡æ•°ã®ç©ºç™½ã‚’1ã¤ã«ã¾ã¨ã‚ã‚‹
        title = re.sub(r'\s+', ' ', title)
        
        # å‰å¾Œã®ç©ºç™½ã‚’å‰Šé™¤
        title = title.strip()
        
        # é•·ã™ãã‚‹å ´åˆã¯åˆ‡ã‚Šè©°ã‚
        if len(title) > 200:
            title = title[:197] + "..."
        
        return title
    
    def get_job_categories(self, jobs: List[Dict[str, str]]) -> Dict[str, int]:
        """
        æ±‚äººã®ã‚«ãƒ†ã‚´ãƒªåˆ¥é›†è¨ˆã‚’å–å¾—
        """
        categories = {
            'ã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢ç³»': 0,
            'ãƒ“ã‚¸ãƒã‚¹ç³»': 0,
            'è£½é€ ãƒ»ç”Ÿç”£ç³»': 0,
            'ITãƒ»ãƒ‡ã‚¸ã‚¿ãƒ«ç³»': 0,
            'ãã®ä»–': 0
        }
        
        for job in jobs:
            title = job['title'].lower()
            
            if any(keyword in title for keyword in ['ã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢', 'é–‹ç™º', 'è¨­è¨ˆ', 'æŠ€è¡“']):
                categories['ã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢ç³»'] += 1
            elif any(keyword in title for keyword in ['å–¶æ¥­', 'ä¼ç”»', 'äººäº‹', 'æ¡ç”¨', 'ãƒãƒ¼ã‚±ãƒ†ã‚£ãƒ³ã‚°']):
                categories['ãƒ“ã‚¸ãƒã‚¹ç³»'] += 1
            elif any(keyword in title for keyword in ['è£½é€ ', 'ç”Ÿç”£', 'å“è³ª', 'çµ„ç«‹']):
                categories['è£½é€ ãƒ»ç”Ÿç”£ç³»'] += 1
            elif any(keyword in title for keyword in ['it', 'ã‚·ã‚¹ãƒ†ãƒ ', 'ai', 'dx', 'ãƒ‡ã‚¸ã‚¿ãƒ«']):
                categories['ITãƒ»ãƒ‡ã‚¸ã‚¿ãƒ«ç³»'] += 1
            else:
                categories['ãã®ä»–'] += 1
        
        return categories
